{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663af44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d839a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6861d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import signal\n",
    "import re\n",
    "\n",
    "def get_gpu_processes():\n",
    "    result = subprocess.run(['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader'], stdout=subprocess.PIPE)\n",
    "    output = result.stdout.decode('utf-8')\n",
    "    pids = [int(pid) for pid in output.strip().split('\\n') if pid]\n",
    "    return pids\n",
    "\n",
    "def kill_processes(pids):\n",
    "    for pid in pids:\n",
    "        try:\n",
    "            os.kill(pid, signal.SIGKILL)\n",
    "            print(f\"Process {pid} has been killed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not kill process {pid}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gpu_pids = get_gpu_processes()\n",
    "    if gpu_pids:\n",
    "\n",
    "        print(f\"Found processes running on GPU: {gpu_pids}\")\n",
    "        kill_processes(gpu_pids)\n",
    "    else:\n",
    "        print(\"No GPU processes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df135eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab899afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.data_decoders.tf_example_decoder import TfExampleDecoder\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.sessions import Session\n",
    "import tensorflow as tf\n",
    "from object_detection import model_lib_v2\n",
    "import neptune.new as neptune\n",
    "from collections import defaultdict\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "def run_training_and_evaluation(run,config_file, model_dir):\n",
    "    config_dir = \"/home/rk42218/Building_Detection/Config_v2/\"\n",
    "    pipeline_config_path = os.path.join(config_dir, config_file)\n",
    "    model_dir = model_dir\n",
    "    print(pipeline_config_path)\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        print(\"Starting training...\")\n",
    "        model_lib_v2.train_loop(\n",
    "            pipeline_config_path=pipeline_config_path,\n",
    "            model_dir=model_dir,\n",
    "            use_tpu=False,\n",
    "            num_steps = 2000,\n",
    "            checkpoint_every_n=200,  \n",
    "            record_summaries=True,\n",
    "            neptune_run=run,\n",
    "        )\n",
    "\n",
    "def run_evaluation(run,pipeline_config_path, model_dir):\n",
    "    print(\"Starting Evaluation...\")\n",
    "    session = Session()\n",
    "    adapter = HTTPAdapter(pool_connections=20, pool_maxsize=20)\n",
    "    session.mount('https://app.neptune.ai', adapter)\n",
    "\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n",
    "    model_config = configs['model']\n",
    "    detection_model = model_builder.build(model_config=model_config, is_training=False) \n",
    "\n",
    "    \n",
    "    feature_description = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/class/text': tf.io.VarLenFeature(tf.string),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/mask': tf.io.VarLenFeature(tf.string),  \n",
    "    }\n",
    "\n",
    "    def parse_tfrecord_fn(example_proto):\n",
    "        example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.image.resize(image, [1024, 1024])  \n",
    "        if 'image/object/mask' in example:\n",
    "            masks = tf.sparse.to_dense(example['image/object/mask'], default_value='')\n",
    "            masks = tf.map_fn(lambda x: tf.image.decode_png(x, channels=1), masks, dtype=tf.uint8)\n",
    "\n",
    "        return image, example['image/object/bbox/xmin'], example['image/object/bbox/xmax'], example['image/object/bbox/ymin'], example['image/object/bbox/ymax'], example['image/object/class/label']\n",
    "\n",
    "    test_record_path = '/home/rk42218/DATA_SET_1024/data_split/val/val.record'\n",
    "    test_dataset = tf.data.TFRecordDataset(test_record_path)\n",
    "    test_dataset = test_dataset.map(parse_tfrecord_fn)\n",
    "    test_dataset = test_dataset.batch(1)  \n",
    "\n",
    "    @tf.function\n",
    "    def detect_fn(image_tensor):\n",
    "        _, shapes = detection_model.preprocess(image_tensor)\n",
    "        prediction_dict = detection_model.predict(image_tensor, shapes)\n",
    "        detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "        return detections\n",
    "\n",
    "    def compute_iou(boxes1, boxes2):\n",
    "        ymin1, xmin1, ymax1, xmax1 = tf.split(boxes1, 4, axis=-1)\n",
    "        ymin2, xmin2, ymax2, xmax2 = tf.split(boxes2, 4, axis=-1)\n",
    "\n",
    "        inter_ymin = tf.maximum(ymin1, tf.transpose(ymin2))\n",
    "        inter_xmin = tf.maximum(xmin1, tf.transpose(xmin2))\n",
    "        inter_ymax = tf.minimum(ymax1, tf.transpose(ymax2))\n",
    "        inter_xmax = tf.minimum(xmax1, tf.transpose(xmax2))\n",
    "\n",
    "        inter_area = tf.maximum(inter_ymax - inter_ymin, 0) * tf.maximum(inter_xmax - inter_xmin, 0)\n",
    "        area1 = (ymax1 - ymin1) * (xmax1 - xmin1)\n",
    "        area2 = (ymax2 - ymin2) * (xmax2 - xmin2)\n",
    "        union_area = area1 + tf.transpose(area2) - inter_area\n",
    "        return inter_area / union_area\n",
    "\n",
    "    def calculate_average_precision(recalls, precisions):\n",
    "        mrec = [0] + recalls + [1]\n",
    "        mpre = [0] + precisions + [0]\n",
    "        for i in range(len(mpre) - 2, -1, -1):\n",
    "            mpre[i] = max(mpre[i], mpre[i + 1])\n",
    "        area = 0.0\n",
    "        for i in range(1, len(mrec)):\n",
    "            area += (mrec[i] - mrec[i - 1]) * mpre[i]\n",
    "        return area\n",
    "\n",
    "    def calculate_f1_score(precision, recall):\n",
    "        return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    def calculate_precision_recall(gt_boxes, pred_boxes, iou_threshold=0.1):\n",
    "        if not pred_boxes.shape[0] or not gt_boxes.shape[0]:\n",
    "            return 0.0, 0.0  \n",
    "\n",
    "        ious = compute_iou(tf.expand_dims(pred_boxes, 1), tf.expand_dims(gt_boxes, 0))\n",
    "        best_iou = tf.reduce_max(ious, axis=2)\n",
    "        best_gt_idx = tf.argmax(ious, axis=2)\n",
    "        true_positives = tf.reduce_sum(tf.cast(best_iou >= iou_threshold, tf.float32), axis=1)\n",
    "        false_positives = tf.reduce_sum(tf.cast(best_iou < iou_threshold, tf.float32), axis=1)\n",
    "        false_negatives = tf.cast(tf.shape(gt_boxes)[0], tf.float32) - true_positives\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        return tf.reduce_mean(precision).numpy(), tf.reduce_mean(recall).numpy() \n",
    "\n",
    "    iou_thresholds = [0.5, 0.75, 0.85] \n",
    "    num_classes = 2  \n",
    "    def run_detection_for_all_checkpoints(ckpt_dir):\n",
    "        ckpt_files = [file for file in os.listdir(ckpt_dir) if file.endswith('.index')]\n",
    "        ckpt_files.sort(key=lambda x: int(x.split('-')[1].split('.')[0]))\n",
    "        for ckpt_file in ckpt_files:\n",
    "            print(\"Evaluating checkpoint:\", ckpt_file)\n",
    "            ckpt_path = os.path.join(ckpt_dir, ckpt_file[:-6])  # Remove '.index' extension\n",
    "            ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "            ckpt.restore(ckpt_path).expect_partial()\n",
    "\n",
    "            mean_iou_metric_path = \"metrics/mean_iou\"\n",
    "            average_precision_metric_path = \"metrics/average_precision\"\n",
    "\n",
    "            run[\"metrics/checkpoint_marker\"].log(\"Checkpoint: \" + ckpt_file[:-6])\n",
    "\n",
    "            all_iou_scores = []\n",
    "            all_precisions = []\n",
    "            all_recalls = []\n",
    "\n",
    "            batch_count = 0  # Initialize batch counter\n",
    "            for image, ymin, xmin, ymax, xmax, labels in test_dataset:\n",
    "                detections = detect_fn(image)\n",
    "                predicted_boxes = detections['detection_boxes'][0]\n",
    "                predicted_scores = detections['detection_scores'][0]\n",
    "\n",
    "                indices = tf.where(predicted_scores > 0.01)\n",
    "                filtered_boxes = tf.gather(predicted_boxes, indices)\n",
    "\n",
    "                ymin_dense = tf.sparse.to_dense(ymin, default_value=0)\n",
    "                xmin_dense = tf.sparse.to_dense(xmin, default_value=0)\n",
    "                ymax_dense = tf.sparse.to_dense(ymax, default_value=0)\n",
    "                xmax_dense = tf.sparse.to_dense(xmax, default_value=0)\n",
    "                gt_boxes = tf.stack([ymin_dense, xmin_dense, ymax_dense, xmax_dense], axis=-1)\n",
    "\n",
    "                if tf.shape(filtered_boxes)[0] > 0 and tf.shape(gt_boxes)[0] > 0:\n",
    "                    precision, recall = calculate_precision_recall(gt_boxes, filtered_boxes)\n",
    "                    f1_score = calculate_f1_score(precision, recall)\n",
    "                    iou_scores = compute_iou(gt_boxes, filtered_boxes)\n",
    "                    all_iou_scores.extend(iou_scores.numpy().tolist())\n",
    "                    all_precisions.append(precision)\n",
    "                    all_recalls.append(recall)\n",
    "                    run[f\"metrics/{ckpt_file[:-6]}/precision\"].log(precision)\n",
    "                    run[f\"metrics/{ckpt_file[:-6]}/recall\"].log(recall)\n",
    "                    run[f\"metrics/{ckpt_file[:-6]}/f1_score\"].log(f1_score)\n",
    "                    if iou_scores.numpy().size > 0:\n",
    "                        run[f\"metrics/{ckpt_file[:-6]}/iou_scores\"].log(iou_scores.numpy().tolist())\n",
    "\n",
    "                batch_count += 1  # Increment batch count\n",
    "                print(f\"Processed batch {batch_count} for checkpoint {ckpt_file[:-6]} for {pipeline_config_path}\")\n",
    "\n",
    "            print(f\"Total batches processed for checkpoint {ckpt_file[:-6]}: {batch_count} for {pipeline_config_path}\")\n",
    "\n",
    "            if all_iou_scores:\n",
    "                flat_iou_scores = [item for sublist in all_iou_scores for item in sublist]\n",
    "                numeric_iou_scores = [score for score in flat_iou_scores if isinstance(score, (int, float))]\n",
    "                mean_iou = sum(numeric_iou_scores) / len(numeric_iou_scores) if numeric_iou_scores else 0.0\n",
    "                average_precision = calculate_average_precision(all_recalls, all_precisions)\n",
    "                run[mean_iou_metric_path].log(mean_iou)\n",
    "                run[average_precision_metric_path].log(average_precision)\n",
    "\n",
    "        run[\"model/config\"].log(str(pipeline_config_path))\n",
    "\n",
    "    run_detection_for_all_checkpoints(model_dir)\n",
    "        \n",
    "def main():\n",
    "    neptune_project= \"ruthikkale27/Building-Detection-Train-Eval01\"\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3ZjllMWM4My1iNzFkLTQ3ZDgtOGU4NC1kN2Y2YmZjMTVmM2QifQ==\"\n",
    "    config_dir = \"/home/rk42218/Building_Detection/Config_v2/\"\n",
    "    model_dir_path = \"/scratch/rk42218/Building_Detection_scratch/training_output_new_Config_v2/\"\n",
    "    test_record_path = '/home/rk42218/DATA_SET_1024/data_split/val/val.record'\n",
    "\n",
    "    config_to_run_id = {\n",
    "        \"4000_32_5e6_v2.config\": \"TRAIN-85\",\n",
    "        \"4000_32_5e7_v2.config\": \"TRAIN-84\",\n",
    "        \"2000_32_2e6_v2.config\": \"TRAIN-83\",\n",
    "        \"2000_32_2e7_v2.config\": \"TRAIN-82\",\n",
    "        \"2000_32_5e7_v1.config\": \"TRAIN-81\",\n",
    "        \"2000_32_5e6_v1.config\": \"TRAIN-80\",\n",
    "        \"4000_32_2e7_v1.config\": \"TRAIN-79\",\n",
    "        \"4000_32_2e6_v1.config\": \"TRAIN-78\",\n",
    "        \"4000_32_2e6_v2.config\": \"TRAIN-77\",\n",
    "        \"4000_32_2e7_v2.config\": \"TRAIN-76\",\n",
    "        \"2000_32_5e6_v2.config\": \"TRAIN-75\",\n",
    "        \"2000_32_5e7_v2.config\": \"TRAIN-74\",\n",
    "        \"2000_32_2e7_v1.config\": \"TRAIN-73\",\n",
    "        \"2000_32_2e6_v1.config\": \"TRAIN-72\",\n",
    "        \"4000_32_5e7_v1.config\": \"TRAIN-71\",\n",
    "        \"4000_32_5e6_v1.config\": \"TRAIN-70\"\n",
    "    }\n",
    "\n",
    "\n",
    "    config_files = os.listdir(config_dir)\n",
    "    for config_file in config_files:\n",
    "        if config_file.endswith('.config'):\n",
    "            run_id = config_to_run_id.get(config_file)\n",
    "            print(f\"Run ID for {config_file}: {run_id}\")  # Debug output\n",
    "\n",
    "            if run_id:\n",
    "                run = neptune.init_run(\n",
    "                    project=neptune_project,\n",
    "                    api_token=api_token,\n",
    "                    with_id=run_id  # Resume an existing run\n",
    "                )\n",
    "            else:\n",
    "                run = neptune.init_run(\n",
    "                    project=neptune_project,\n",
    "                    api_token=api_token,\n",
    "                    tags=['new_run', config_file[:-7]]  # Tagging new run with config name\n",
    "                )\n",
    "                print(f\"Starting new Neptune run for {config_file}\")\n",
    "\n",
    "            pipeline_config_path = os.path.join(config_dir, config_file)\n",
    "            model_dir = os.path.join(model_dir_path, config_file[:-7])\n",
    "            print(\"Processing config:\", config_file)\n",
    "            print(\"Model directory:\", model_dir)\n",
    "\n",
    "            run_training_and_evaluation(run, pipeline_config_path, model_dir)\n",
    "            #run_evaluation(run, pipeline_config_path, model_dir)\n",
    "\n",
    "            run.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
